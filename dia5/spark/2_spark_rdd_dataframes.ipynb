{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","!pip install -q pyspark"],"metadata":{"id":"EsufO7-RIr_a","executionInfo":{"status":"ok","timestamp":1768660536032,"user_tz":300,"elapsed":18686,"user":{"displayName":"Cesar Cuela","userId":"12195114202184066508"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# Clase 1: Creando y entendiendo RDD y DataFrames\n","\n","## ¿Qué son los objetos RDD?\n","\n","- **RDD (Resilient Distributed Dataset)** es la unidad básica de datos en Apache Spark.\n","- Son **estructuras inmutables** y **distribuidas** que permiten realizar transformaciones y acciones en grandes conjuntos de datos.\n","- Soportan **tolerancia a fallos** mediante linaje (registro de operaciones).\n","- Dos tipos de operaciones principales:\n","  - **Transformaciones** (lazy): `map()`, `filter()`, `flatMap()`, etc.\n","  - **Acciones** (trigger ejecución): `collect()`, `count()`, `take()`, etc.\n","- Ideal cuando necesitas **control fino** del procesamiento o trabajar con estructuras no estructuradas.\n","\n"],"metadata":{"id":"g8tnohF2S6Ag"}},{"cell_type":"code","source":["from pyspark import SparkContext\n","sc = SparkContext()\n","\n","rdd = sc.parallelize([1, 2, 3, 4, 5])\n","squared = rdd.map(lambda x: x ** 2)\n","print(squared.collect())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3H2vOO9S8q3","outputId":"7bc92da9-7ce5-4022-e1f1-e6bdf03a7b84","executionInfo":{"status":"ok","timestamp":1768660577183,"user_tz":300,"elapsed":13217,"user":{"displayName":"Cesar Cuela","userId":"12195114202184066508"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 4, 9, 16, 25]\n"]}]},{"cell_type":"markdown","source":["## ¿Qué son los objetos DataFrames?\n","\n","**DataFrame** es una colección distribuida de datos **organizados en columnas**, similar a una tabla de una base de datos o un DataFrame de Pandas.\n","\n","- Internamente se basa en **RDD**, pero proporciona una **API más expresiva y optimizada**.\n","- Permite realizar **consultas SQL** y aplicar operaciones de transformación con mayor facilidad.\n","- Se pueden crear desde diversas fuentes: **JSON, CSV, Parquet, bases de datos**, entre otros.\n"],"metadata":{"id":"zxZ_AMxzS-QP"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"Ejemplo\").getOrCreate()\n","\n","data = [(\"Juan\", 30), (\"Ana\", 25), (\"Pedro\", 40)]\n","df = spark.createDataFrame(data, [\"Nombre\", \"Edad\"])\n","df.show()\n"],"metadata":{"id":"6kJcbBYiTFSv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5f51003a-8ae5-4a27-bf80-d5932da278df","executionInfo":{"status":"ok","timestamp":1768660646809,"user_tz":300,"elapsed":9216,"user":{"displayName":"Cesar Cuela","userId":"12195114202184066508"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+----+\n","|Nombre|Edad|\n","+------+----+\n","|  Juan|  30|\n","|   Ana|  25|\n","| Pedro|  40|\n","+------+----+\n","\n"]}]},{"cell_type":"markdown","source":["## Comparación entre RDDs y DataFrames\n","\n","| Característica       | RDD                             | DataFrame                          |\n","|----------------------|----------------------------------|------------------------------------|\n","| Nivel de abstracción | Bajo                             | Alto                               |\n","| Tipo de datos        | Objetos de Python, Java, etc.    | Columnas con esquema estructurado  |\n","| Optimización         | Manual                           | Automática (Catalyst Optimizer)    |\n","| API                  | Funcional (`map`, `filter`)      | Declarativa (`select`, `groupBy`)  |\n","| Soporte para SQL     | No                               | Sí                                 |\n","| Rendimiento          | Generalmente menor               | Generalmente mayor                 |\n","| Legibilidad del código | Menor                          | Mayor                              |\n","| Interoperabilidad    | Limitada                         | Alta (con BI, SQL, ML, etc.)       |\n","\n","> ✅ **Conclusión:** Para análisis estructurado, usa DataFrames. Usa RDD solo si necesitas bajo nivel o control total de las transformaciones.\n"],"metadata":{"id":"8B-PknYtTR7P"}},{"cell_type":"markdown","source":[],"metadata":{"id":"-R13vQ7WT6OA"}},{"cell_type":"markdown","source":["## ¿Cuándo usar RDD?\n","\n","- Necesitas **control de bajo nivel** del procesamiento.\n","- Realizas **operaciones complejas** que no están disponibles en DataFrame.\n","- Trabajas con **datos no estructurados** o con lógica de transformación personalizada.\n","- Prefieres un enfoque **funcional** tipo Scala o Python usando `map`, `reduce`, etc.\n","\n","---\n","\n","## ¿Cuándo usar DataFrames?\n","\n","- Cuando trabajas con **datos estructurados** (columnas definidas).\n","- Requieres **consultas SQL** o análisis de datos tabulares.\n","- Buscas **mejor rendimiento** gracias a optimizaciones internas como **Catalyst**.\n","- Quieres aprovechar **integración con herramientas de BI** (Power BI, Tableau, etc.).\n","- Deseas escribir **menos código** y que sea más **legible**.\n"],"metadata":{"id":"Zl70rRuwTde3"}}]}